{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "\n",
    "import google.generativeai as genai\n",
    "from qdrant_client import QdrantClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIQueryAgent:\n",
    "    \"\"\"\n",
    "    Um agente que recebe perguntas em linguagem natural,\n",
    "    traduz para SQL, consulta um SQLite, sumariza os resultados\n",
    "    e busca notícias relacionadas no Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sqlite_path: str,\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        qdrant_collection: str = \"sars_cov_news\",\n",
    "        embedding_model: str = \"models/embedding-001\",\n",
    "        sql_model: str = \"gemini-2.0-flash-lite\",\n",
    "        summarization_model: str = \"gemini-2.0-flash-lite\",\n",
    "        final_model: str = \"gemini-2.5-flash\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param sqlite_path: caminho para o arquivo .db do SQLite\n",
    "        :param qdrant_url: URL do servidor Qdrant\n",
    "        :param qdrant_collection: nome da coleção onde estão as notícias\n",
    "        :param embedding_model: modelo de embedding do Gemini\n",
    "        :param sql_model: modelo de LLM para tradução NL→SQL\n",
    "        :param summarization_model: modelo de LLM para resumo de resultados\n",
    "        :param final_model: modelo de LLM para montar a resposta final\n",
    "        \"\"\"\n",
    "        self.sqlite_path = sqlite_path\n",
    "        self.qdrant = QdrantClient(url=qdrant_url)\n",
    "        self.qdrant_collection = qdrant_collection\n",
    "        self.embedding_model = embedding_model\n",
    "        self.sql_model = sql_model\n",
    "        self.summarization_model = summarization_model\n",
    "        self.final_model = final_model\n",
    "\n",
    "        genai.configure(api_key=os.environ.get(\"GOOGLE_API_KEY\"))\n",
    "\n",
    "    def _execute_sql(self, sql: str) -> list[tuple]:\n",
    "        \"\"\"Executa a query SQL no banco SQLite e retorna todas as linhas.\"\"\"\n",
    "        # Regex to capture the SQL between the code fences\n",
    "        if match := re.search(r\"```(?:\\w+)?\\n([\\s\\S]*?)\\n```\", sql):\n",
    "            # Replace the table name after FROM with 'srag_cases'\n",
    "            sql = re.sub(r\"(?i)(FROM\\s+)(\\w+)\", r\"\\1srag_cases\", match.group(1).strip())\n",
    "\n",
    "        with sqlite3.connect(self.sqlite_path) as conn:\n",
    "            cur = conn.cursor()\n",
    "            cur.execute(sql)\n",
    "            return cur.fetchall()\n",
    "\n",
    "    def _llm_chat(self, model: str, messages: list[dict]) -> str:\n",
    "        \"\"\"Chama o endpoint de chat do Gemini.\"\"\"\n",
    "        return (\n",
    "            genai.GenerativeModel(model)\n",
    "            .generate_content(\n",
    "                \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages])\n",
    "            )\n",
    "            .text.strip()\n",
    "        )\n",
    "\n",
    "    def _natural_to_sql(self, question: str) -> str:\n",
    "        \"\"\"Traduz a pergunta em linguagem natural para uma consulta SQL válida.\"\"\"\n",
    "        return self._llm_chat(\n",
    "            self.sql_model,\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a SQL query generator for SQLite.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Translate this question into SQL:\\n{question}\",\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _summarize(self, question: str, results: list[tuple]) -> str:\n",
    "        \"\"\"Gera um pequeno resumo explicativo dos resultados obtidos.\"\"\"\n",
    "        return self._llm_chat(\n",
    "            self.summarization_model,\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You summarize table results.\"},\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"Question:\\n\"\n",
    "                        + question\n",
    "                        + \"\\nResults (list of tuples):\\n\"\n",
    "                        + repr(results)\n",
    "                        + \"\\nWrite a short paragraph explaining what those data shows.\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    def _get_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"Gera embedding para um texto usando a API do Gemini.\"\"\"\n",
    "        return genai.embed_content(model=self.embedding_model, content=text)[\n",
    "            \"embedding\"\n",
    "        ]\n",
    "\n",
    "    def _search_news(self, embedding: list[float], limit: int = 5) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Busca notícias relacionadas no Qdrant, usando similaridade de coseno.\n",
    "        Retorna lista de hits (id, payload).\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\"id\": hit.id, \"score\": hit.score, \"payload\": hit.payload}\n",
    "            for hit in self.qdrant.search(\n",
    "                collection_name=self.qdrant_collection,\n",
    "                query_vector=embedding,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"Ciclo principal: recebe pergunta, executa SQL, resume, busca notícias e devolve resposta final.\"\"\"\n",
    "        sql = self._natural_to_sql(question)\n",
    "        summary = self._summarize(question, self._execute_sql(sql))\n",
    "        return self._llm_chat(\n",
    "            self.final_model,\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an agent that answers questions based on tabular data and related news.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Question: {question}\\n\\n\"\n",
    "                    f\"SQL generated:\\n{sql}\\n\\n\"\n",
    "                    f\"Data Summary:\\n{summary}\\n\\n\"\n",
    "                    f\"Related news (id and title):\\n\"\n",
    "                    + \"\\n\".join(\n",
    "                        f\"- {hit['id']}: {hit['payload'].get('title', '(sem título)')}\"\n",
    "                        for hit in self._search_news(self._get_embedding(summary))\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AIQueryAgent(sqlite_path=\"../data/interim/srag_2019_2024.db\")\n",
    "print(agent.ask(\"what was the month with more cases of sars in 2019\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-indicium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
