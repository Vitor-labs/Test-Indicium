{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\VDUART10\\Desktop\\Tests\\test_indicium\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Callable\n",
    "\n",
    "from camelot import read_pdf\n",
    "from dotenv import load_dotenv\n",
    "from httpx import HTTPStatusError, get\n",
    "from pandas import (\n",
    "    Categorical,\n",
    "    DataFrame,\n",
    "    Series,\n",
    "    concat,\n",
    "    isna,\n",
    "    read_csv,\n",
    "    set_option,\n",
    "    to_datetime,\n",
    "    to_numeric,\n",
    ")\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sqlalchemy.types import TIMESTAMP\n",
    "from tabulate import tabulate\n",
    "\n",
    "load_dotenv()  # i don't use dotenv, but in case you didn't config vscode previously\n",
    "\n",
    "set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo Schema de dados do dicionario de dados\n",
    "\n",
    "Para este caso em especifico eu não fiz nenhuma estrutura de processamento de texto por que o intuito era extrair a informação de forma rapida.\n",
    "\n",
    "Poderia-se criar uma pipeline de extração de conteudo cru com uma lib como `pdfplumber` extraindo texto bruto e iterando sobre coletando nome, tipo e info das colunas + metadados opcionais, mas optei por agilidade na solução. Isso aqui teoricamente é pra ser feito uma vez só, então não gastei muitos recursos encima, como faria numa solução real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_markdown_table(df: DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a clean markdown table format\n",
    "    \"\"\"\n",
    "    df = df.dropna(axis=1, how=\"all\").fillna(\"\")\n",
    "\n",
    "    # Clean data by removing excessive whitespace\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return tabulate(df, headers=\"keys\", tablefmt=\"pipe\", showindex=False)\n",
    "\n",
    "\n",
    "def is_continuation_row(row: Series) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a row appears to be a continuation from a previous page.\n",
    "\n",
    "    A continuation row is identified when:\n",
    "    - Columns 0-2 are empty/null\n",
    "    - Columns 3-5 contain data\n",
    "\n",
    "    Args:\n",
    "        row (Series): A pandas Series representing a single row from a dataframe\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the row appears to be a continuation row, False otherwise\n",
    "\n",
    "    Note:\n",
    "        Requires at least 5 columns in the row to perform the check.\n",
    "        Returns False if the row has fewer than 5 columns.\n",
    "    \"\"\"\n",
    "    if len(row) < 5:\n",
    "        return False\n",
    "\n",
    "    return all(  # Check if first 3 columns are empty\n",
    "        isna(row.iloc[j]) or str(row.iloc[j]).strip() == \"\" for j in range(3)\n",
    "    ) and any(  # Check if columns 3-5 have data\n",
    "        not isna(row.iloc[j]) and str(row.iloc[j]).strip() != \"\" for j in range(2, 5)\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_continuation_row(target_df: DataFrame, continuation_row: Series) -> None:\n",
    "    \"\"\"\n",
    "    Merge a continuation row with the last row of the target dataframe.\n",
    "\n",
    "    The function modifies the target dataframe in-place by:\n",
    "    - Filling empty cells in the last row with data from the continuation row\n",
    "    - Concatenating content when both cells contain data\n",
    "\n",
    "    Args:\n",
    "        target_df (DataFrame): The dataframe whose last row will be updated\n",
    "        continuation_row (Series): The row containing continuation data\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies target_df in-place\n",
    "\n",
    "    Note:\n",
    "        If target_df is empty, the function returns without making changes.\n",
    "        Only processes columns that exist in both the target dataframe and continuation row.\n",
    "    \"\"\"\n",
    "    if len(target_df) == 0:\n",
    "        return\n",
    "\n",
    "    last_row_idx = target_df.index[-1]\n",
    "\n",
    "    for col_idx in range(len(continuation_row)):\n",
    "        if col_idx >= len(target_df.columns):\n",
    "            break\n",
    "\n",
    "        col_name = target_df.columns[col_idx]\n",
    "        continuation_value = continuation_row.iloc[col_idx]\n",
    "\n",
    "        # Skip if continuation cell is empty\n",
    "        if isna(continuation_value) or str(continuation_value).strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        existing_value = target_df.loc[last_row_idx, col_name]\n",
    "\n",
    "        # If target cell is empty, use continuation value\n",
    "        if isna(existing_value) or str(existing_value).strip() == \"\":\n",
    "            target_df.loc[last_row_idx, col_name] = continuation_value\n",
    "        else:\n",
    "            # Both have content, concatenate with space\n",
    "            existing = str(existing_value).strip()\n",
    "            new_content = str(continuation_value).strip()\n",
    "            target_df.loc[last_row_idx, col_name] = f\"{existing} {new_content}\"\n",
    "\n",
    "\n",
    "def process_single_dataframe(df: DataFrame, previous_df: DataFrame | None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process a single dataframe, checking for continuation rows and handling merging.\n",
    "\n",
    "    If the first row of the dataframe is identified as a continuation row and there's\n",
    "    a previous dataframe available, the continuation row is merged with the last row\n",
    "    of the previous dataframe and removed from the current dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The dataframe to process\n",
    "        previous_df (Optional[DataFrame]): The previous dataframe for potential merging.\n",
    "                                            None if this is the first dataframe.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: A copy of the processed dataframe with continuation rows removed\n",
    "                     if they were merged with the previous dataframe\n",
    "\n",
    "    Note:\n",
    "        If df is empty, returns an empty copy.\n",
    "        The previous_df is modified in-place if a merge occurs.\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return df.copy()\n",
    "\n",
    "    current_df = df.copy()\n",
    "    first_row = current_df.iloc[0]\n",
    "\n",
    "    # Check if first row is a continuation row and we have a previous dataframe\n",
    "    if is_continuation_row(first_row) and previous_df is not None:\n",
    "        # Merge with previous dataframe\n",
    "        merge_continuation_row(previous_df, first_row)\n",
    "        # Remove the merged row from current dataframe\n",
    "        current_df = current_df.iloc[1:].copy()\n",
    "\n",
    "    return DataFrame(current_df)\n",
    "\n",
    "\n",
    "def merge_split_rows(dfs: list[DataFrame]) -> list[DataFrame]:\n",
    "    \"\"\"\n",
    "    Merge rows that are split across PDF pages in a list of dataframes.\n",
    "\n",
    "    This function processes a list of dataframes (typically representing tables from\n",
    "    different PDF pages) and merges rows that were split across page boundaries.\n",
    "    Split rows are identified using the continuation row detection logic.\n",
    "\n",
    "    Args:\n",
    "        dfs (List[DataFrame]): List of dataframes to process, typically one per PDF page\n",
    "\n",
    "    Returns:\n",
    "        List[DataFrame]: List of processed dataframes with split rows merged.\n",
    "                           Empty dataframes are excluded from the result.\n",
    "\n",
    "    Note:\n",
    "        The function processes dataframes sequentially, so continuation rows are only\n",
    "        merged with the immediately preceding dataframe in the list.\n",
    "        Original dataframes are not modified; copies are created for processing.\n",
    "\n",
    "    Example:\n",
    "        >>> dfs = [df_page1, df_page2, df_page3]\n",
    "        >>> merged_dfs = merge_split_rows(dfs)\n",
    "        >>> final_df = concat(merged_dfs, ignore_index=True)\n",
    "    \"\"\"\n",
    "    if not dfs:\n",
    "        return []\n",
    "\n",
    "    processed_dfs: list[DataFrame] = []\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        processed_df = process_single_dataframe(\n",
    "            df, processed_dfs[-1] if processed_dfs else None\n",
    "        )\n",
    "        if len(processed_df) > 0:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "    return processed_dfs\n",
    "\n",
    "\n",
    "def clean_markdown_text(text):\n",
    "    \"\"\"\n",
    "    Clean and enhance the markdown text for better LLM processing\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # Trim whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip completely empty lines except single newlines\n",
    "        if line != \"\" or (cleaned_lines and cleaned_lines[-1] != \"\"):\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    content = read_pdf(\n",
    "        \"../docs/dicionario-de-dados-2019-a-2025.pdf\", pages=\"all\", line_scale=40\n",
    "    )\n",
    "    for df in (dfs := [table.df for table in content][1:]):\n",
    "        df.columns = dfs[0].iloc[0]  # Set the first row as column names\n",
    "\n",
    "    dfs[0].drop(index=0, inplace=True)\n",
    "    df_list = [\n",
    "        \"## Concatenated Table\\n\\n\"\n",
    "        + df_to_markdown_table(\n",
    "            all_df := concat(merge_split_rows(dfs), ignore_index=True)\n",
    "        )\n",
    "        + \"\\n\"\n",
    "    ]\n",
    "    all_df[\"Tipo\"] = all_df[\"Tipo\"].str.replace(\"\\n\", \"\")\n",
    "    all_df.to_csv(\"../docs/data_dict.csv\", index=False)\n",
    "    print(\"Successfully extracted markdown tables from the PDF.\")\n",
    "    with open(\"../docs/data_dict.md\", \"w\") as f:\n",
    "        f.write(clean_markdown_text(\"\\n\".join(df_list)))\n",
    "    print(\"Markdown conversion completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting tables: {e}\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with (\n",
    "    open(\"../docs/data_dict.csv\", newline=\"\", encoding=\"utf-8\") as fin,\n",
    "    open(\"../docs/data_dict_formated.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as fout,\n",
    "):\n",
    "    reader = csv.reader(fin)\n",
    "    writer = csv.writer(fout)\n",
    "    # Cabeçalho unificado\n",
    "    writer.writerow(\n",
    "        [\n",
    "            \"field_name\",\n",
    "            \"data_type\",\n",
    "            \"category\",\n",
    "            \"description\",\n",
    "            \"characteristics\",\n",
    "            \"db_field\",\n",
    "        ]\n",
    "    )\n",
    "    for row in reader:\n",
    "        if not any(cell.strip() for cell in row):\n",
    "            continue\n",
    "        # Junta colunas que ficaram “quebradas” em descrição\n",
    "        # Supondo que DBF (última coluna) nunca contenha vírgula\n",
    "        *middle, dbf = row\n",
    "        # As três primeiras colunas fixas\n",
    "        raw_name, raw_type, raw_cat = middle[:3]\n",
    "        rest = middle[3:]\n",
    "        # Descrição + características podem estar mescladas\n",
    "        # Separa descrição de características por padrão de texto\n",
    "        if '\"' in (text := \" \".join(cell.strip() for cell in rest if cell.strip())):\n",
    "            desc, _, chars = text.partition(\"Campo\")\n",
    "            chars = \"Campo\" + chars if chars else \"\"\n",
    "        else:\n",
    "            desc, chars = text, \"\"\n",
    "        writer.writerow(\n",
    "            [\n",
    "                re.sub(r\"[^a-z0-9]+\", \"_\", raw_name.strip().lower()).strip(\"_\"),\n",
    "                raw_type.replace(\"\\n\", \" \").strip(),\n",
    "                raw_cat.replace(\"\\n\", \" \").strip(),\n",
    "                desc.strip().replace(\"\\n\", \" \"),\n",
    "                chars.strip().replace(\"\\n\", \" \"),\n",
    "                dbf.strip(),\n",
    "            ]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo bases de dados e dividindo entre ambientes\n",
    "- Esse processo aqui eu jogaria esses Data Lake pra ser acessivel ao time.\n",
    "- Vamos fingir q o diretorio data é nosso Lake.\n",
    "- Com um sqlite tambem simulando um postgres, se sobrar tempo eu troco por um Postgres num docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIXED_TYPE_COLS = [  # tá hardcoded, mas é o que tenho por agora\n",
    "    14,\n",
    "    17,\n",
    "    66,\n",
    "    92,\n",
    "    94,\n",
    "    106,\n",
    "    108,\n",
    "    115,\n",
    "    117,\n",
    "    118,\n",
    "    119,\n",
    "    121,\n",
    "    122,\n",
    "    123,\n",
    "    129,\n",
    "    144,\n",
    "    146,\n",
    "    149,\n",
    "    155,\n",
    "    159,\n",
    "    160,\n",
    "    161,\n",
    "    162,\n",
    "    163,\n",
    "    164,\n",
    "    165,\n",
    "    166,\n",
    "    167,\n",
    "    168,\n",
    "    169,\n",
    "    170,\n",
    "    171,\n",
    "    172,\n",
    "    173,\n",
    "    174,\n",
    "    175,\n",
    "    176,\n",
    "    181,\n",
    "    185,\n",
    "    186,\n",
    "    188,\n",
    "    189,\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_type_mapping(data_dict_df: DataFrame) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Enhanced type mapping with better pattern matching and validation.\n",
    "    \"\"\"\n",
    "    type_mapping = {}\n",
    "\n",
    "    for _, row in data_dict_df.iterrows():\n",
    "        col_name = row[\"DBF\"]\n",
    "        col_type = row[\"Tipo\"]\n",
    "\n",
    "        if isna(col_name) or isna(col_type):  # type: ignore\n",
    "            continue\n",
    "\n",
    "        col_type_clean = str(col_type).strip().replace(\" \", \"\")\n",
    "\n",
    "        # Enhanced pattern matching\n",
    "        if re.search(r\"Date|Data\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"datetime64[ns]\"\n",
    "        elif re.search(r\"Number\\(\\d+\\)\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"float64\"\n",
    "        elif re.search(r\"Varchar2?\\(1\\)\", col_type_clean, re.IGNORECASE):\n",
    "            # Single character fields are good candidates for categories\n",
    "            type_mapping[col_name] = \"category\"\n",
    "        elif re.search(r\"Varchar2?\\(\\d+\\)\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"string\"\n",
    "        else:  # Default fallback for malformed or unrecognized types\n",
    "            type_mapping[col_name] = \"string\"\n",
    "\n",
    "    return type_mapping\n",
    "\n",
    "\n",
    "def create_converters(dtype_mapping: dict[str, str]) -> dict[str, Callable]:\n",
    "    \"\"\"\n",
    "    Create converter functions with proper scope handling.\n",
    "    \"\"\"\n",
    "    converters = {}\n",
    "\n",
    "    for col_name, dtype in dtype_mapping.items():\n",
    "        if dtype == \"datetime64[ns]\":\n",
    "            converters[col_name] = lambda x: to_datetime(\n",
    "                x, format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "            )\n",
    "        elif dtype == \"float64\":\n",
    "            converters[col_name] = lambda x: to_numeric(x, errors=\"coerce\")\n",
    "        elif dtype == \"category\":\n",
    "            converters[col_name] = lambda x: Categorical(x)\n",
    "        elif dtype == \"string\":\n",
    "            converters[col_name] = lambda x: x.astype(\"string\")\n",
    "\n",
    "    return converters\n",
    "\n",
    "\n",
    "def apply_dtype_conversions(df: DataFrame, dtype_mapping: dict[str, str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply data type conversions to DataFrame.\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "\n",
    "    for col_name, target_dtype in dtype_mapping.items():\n",
    "        if col_name not in df_converted.columns:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if target_dtype == \"datetime64[ns]\":\n",
    "                df_converted[col_name] = to_datetime(\n",
    "                    df_converted[col_name], format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "                )\n",
    "            elif target_dtype == \"float64\":\n",
    "                df_converted[col_name] = to_numeric(\n",
    "                    df_converted[col_name], errors=\"coerce\"\n",
    "                )\n",
    "            elif target_dtype == \"category\":\n",
    "                df_converted[col_name] = df_converted[col_name].astype(\"category\")\n",
    "            elif target_dtype == \"string\":\n",
    "                df_converted[col_name] = df_converted[col_name].astype(\"string\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: Could not convert column {col_name} to {target_dtype}: {e}\"\n",
    "            )\n",
    "\n",
    "    return df_converted\n",
    "\n",
    "\n",
    "def optimize_for_target_format(df: DataFrame, target_format: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Optimize DataFrame types for Parquet or SQLite.\n",
    "    - parquet: converts suitable object cols to 'category'\n",
    "    - sqlite: ensures categories & mixed cols are strings\n",
    "    \"\"\"\n",
    "    df_optimized = df.copy()\n",
    "    target_format = target_format.lower()\n",
    "\n",
    "    # Resolve column names from indices (skip if index > ncols)\n",
    "    mixed_cols = [\n",
    "        df_optimized.columns[i]\n",
    "        for i in MIXED_TYPE_COLS\n",
    "        if i < len(df_optimized.columns)\n",
    "    ]\n",
    "    if target_format == \"parquet\":\n",
    "        for col in mixed_cols:\n",
    "            if df_optimized[col].dtype == \"object\":\n",
    "                # Try to coerce to numeric if possible\n",
    "                coerced = to_numeric(df_optimized[col], errors=\"coerce\")\n",
    "                if coerced.notna().mean() > 0.9:  # >90% convertible\n",
    "                    df_optimized[col] = coerced\n",
    "                else:  # Use category if low cardinality\n",
    "                    if df_optimized[col].nunique() / len(df_optimized) < 0.5:\n",
    "                        df_optimized[col] = df_optimized[col].astype(\"category\")\n",
    "\n",
    "        # Handle remaining object columns (same as your original)\n",
    "        for col in df_optimized.select_dtypes(include=[\"object\"]).columns:\n",
    "            if df_optimized[col].nunique() / len(df_optimized) < 0.5:\n",
    "                df_optimized[col] = df_optimized[col].astype(\"category\")\n",
    "\n",
    "    elif target_format == \"sqlite\":\n",
    "        # SQLite doesn’t support category → convert to string\n",
    "        for col in df_optimized.select_dtypes(include=[\"category\"]).columns:\n",
    "            df_optimized[col] = df_optimized[col].astype(\"string\")\n",
    "        # Force mixed-type columns to string (to avoid type conflicts)\n",
    "        for col in mixed_cols:\n",
    "            df_optimized[col] = df_optimized[col].astype(\"string\")\n",
    "\n",
    "    return df_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_mapping = get_column_type_mapping(all_df)\n",
    "\n",
    "DFS: list[DataFrame] = []\n",
    "for y in range(19, 25):\n",
    "    print(f\"Retriving year 20{y} dataset... \", end=\"\")\n",
    "    DFS.append(\n",
    "        df := apply_dtype_conversions(\n",
    "            read_csv(\n",
    "                f\"https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SRAG/20{y}/INFLUD{y}-26-06-2025.csv\",\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            ),\n",
    "            dtype_mapping,\n",
    "        )\n",
    "    )\n",
    "    df.to_csv(f\"../data/raw/srag_20{y}.csv\", index=False, date_format=\"%Y-%m-%d\")\n",
    "    print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging raw data into one database\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VDUART10\\AppData\\Local\\Temp\\ipykernel_47112\\2390960348.py:3: DtypeWarning: Columns (17,92,94,106,108,115,117,118,119,121,122,123,129,144,146,149,155,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,181,185,186,189) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  merged_df = read_csv(\"../data/interim/srag_2019_2024.csv\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging raw data into one database\")\n",
    "merged_df = concat(DFS, ignore_index=True)\n",
    "merged_df.to_csv(\n",
    "    \"../data/interim/srag_2019_2024.csv\", index=False, date_format=\"%Y-%m-%d\"\n",
    ")\n",
    "df_parquet = optimize_for_target_format(merged_df, \"parquet\")\n",
    "\n",
    "for col in df_parquet.select_dtypes([\"category\"]).columns:\n",
    "    if df_parquet[col].cat.categories.inferred_type not in [\"string\", \"unicode\"]:\n",
    "        df_parquet[col] = df_parquet[col].astype(\"string\").astype(\"category\")\n",
    "\n",
    "df_parquet.to_parquet(  # Não otimizando pq não limpei a base\n",
    "    \"../data/interim/srag_2019_2024.parquet\", index=False, compression=\"snappy\"\n",
    ")\n",
    "with sqlite3.connect(\"../data/interim/srag_2019_2024.db\") as conn:\n",
    "    optimize_for_target_format(merged_df, \"sqlite\").to_sql(\n",
    "        \"srag_cases\",\n",
    "        conn,\n",
    "        if_exists=\"replace\",\n",
    "        index=False,\n",
    "        dtype={\n",
    "            col: TIMESTAMP\n",
    "            for col, dtype in merged_df.dtypes.items()\n",
    "            if dtype == \"datetime64[ns]\"\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando NEWS API e salvando embeddings\n",
    "criei uma unica coleção no qdrant para armazenar artigos de noticias sobre gripe, covid, sars e relacionados.\n",
    "O fluxo é bem simples, apenas para alimentar a base de dados, num caso real teria q ser feito uma tratamento maior nas bases e aplicar criterios de escolha aos artigos como confiabilidade da fonte por exemplo. Segue a descrição:\n",
    "\n",
    "1. dados são coletados usando a NEWs API para retorar artigos (você vai precisar criar uma key)\n",
    "2. os artigos selecionados (tudo, não apliquei pesos ou discarte) são processados e salvos na coleção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAPICollector:\n",
    "    def __init__(self):\n",
    "        self.BASE_URL = \"https://newsapi.org/v2/everything\"\n",
    "        # isso devia estar em um vault ou secret manager, mas vou usar o .env pra simular\n",
    "        self.API_KEY = os.getenv(\"NEWS_API_KEY\")\n",
    "\n",
    "    def fetch_articles(\n",
    "        self,\n",
    "        query: str,\n",
    "        sort_by: str = \"relevancy\",  # Use 'relevancy' instead of 'popularity'\n",
    "        days_back: int = 29,\n",
    "    ) -> list[dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from News API for a specific query\n",
    "        \"\"\"\n",
    "        to_date = datetime.now()\n",
    "        from_date = to_date - timedelta(days=min(days_back, 29))  # Ensure max 29 days\n",
    "        print(f\"Fetching articles from {from_date} to {to_date} for query '{query}'\")\n",
    "        try:\n",
    "            data: dict[str, Any] = (\n",
    "                get(\n",
    "                    self.BASE_URL,\n",
    "                    params={\n",
    "                        \"q\": query,\n",
    "                        \"from\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                        \"to\": to_date.strftime(\"%Y-%m-%d\"),\n",
    "                        \"sortBy\": sort_by,  # 'relevancy', 'popularity', or 'publishedAt'\n",
    "                        \"apiKey\": self.API_KEY,\n",
    "                    },\n",
    "                )\n",
    "                .raise_for_status()\n",
    "                .json()\n",
    "            )\n",
    "\n",
    "            if data[\"status\"] == \"ok\":\n",
    "                return data[\"articles\"]\n",
    "            else:\n",
    "                print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "\n",
    "        except HTTPStatusError as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def preprocess_articles(\n",
    "        self, articles: list[dict[str, str]]\n",
    "    ) -> list[dict[str, str | dict[str, str]]]:\n",
    "        \"\"\"\n",
    "        Clean and preprocess articles for RAG system\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return [\n",
    "                {\n",
    "                    \"id\": f\"news_{hash(article['url'])}\",\n",
    "                    # Combine title, description, and content\n",
    "                    \"text\": f\"{article.get('title', '')} {article.get('description', '')} {article.get('content', '')}\".strip(),\n",
    "                    \"title\": article.get(\"title\", \"\"),\n",
    "                    \"url\": article.get(\"url\", \"\"),\n",
    "                    \"metadata\": {\n",
    "                        \"source_type\": \"news_api\",\n",
    "                        \"topic\": \"sars_cov\",  # static topic for this use case\n",
    "                        \"url\": article.get(\"url\", \"\"),\n",
    "                        \"published_date\": article.get(\"publishedAt\", \"\"),\n",
    "                        \"source_name\": article.get(\"source\", {}).get(\"name\", \"\"),\n",
    "                        \"image_url\": article.get(\"urlToImage\", \"\"),\n",
    "                    },\n",
    "                }\n",
    "                for article in articles\n",
    "                if article.get(\"content\") or article[\"content\"] != \"[Removed]\"\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error preprocessing articles: {e}\")\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantRAGSystem:\n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        collection_name: str = \"sars_cov_news\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "    ):\n",
    "        self.client = QdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "        self.encoder = SentenceTransformer(embedding_model)\n",
    "        self.vector_size = self.encoder.get_sentence_embedding_dimension()\n",
    "        self._create_collection()  # Create collection if it doesn't exist\n",
    "\n",
    "    def _create_collection(self):\n",
    "        \"\"\"Create Qdrant collection for storing embeddings\"\"\"\n",
    "        try:\n",
    "            # Check if collection exists\n",
    "            if self.collection_name not in [\n",
    "                col.name for col in self.client.get_collections().collections\n",
    "            ]:\n",
    "                self.client.create_collection(\n",
    "                    collection_name=self.collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.vector_size, distance=Distance.COSINE\n",
    "                    ),\n",
    "                )\n",
    "                print(f\"Created collection: {self.collection_name}\")\n",
    "            else:\n",
    "                print(f\"Collection {self.collection_name} already exists\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating collection: {e}\")\n",
    "\n",
    "    def add_documents(self, documents: list[dict[str, str | dict[str, str]]]):\n",
    "        \"\"\"Add documents to Qdrant with embeddings\"\"\"\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                # Generate embedding for the text\n",
    "                vector=self.encoder.encode(doc[\"text\"]).tolist(),\n",
    "                payload={\n",
    "                    \"text\": doc[\"text\"],\n",
    "                    \"title\": doc[\"title\"],\n",
    "                    \"url\": doc[\"url\"],\n",
    "                    \"metadata\": doc[\"metadata\"],\n",
    "                },\n",
    "            )\n",
    "            for doc in documents\n",
    "        ]  # Create points for Qdrant\n",
    "        try:  # Batch upload to Qdrant\n",
    "            self.client.upsert(collection_name=self.collection_name, points=points)\n",
    "            print(f\"Successfully added {len(points)} documents to Qdrant\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to Qdrant: {e}\")\n",
    "\n",
    "    def search_similar(self, query: str, limit: int = 5) -> list[dict]:\n",
    "        \"\"\"Search for similar documents using vector similarity\"\"\"\n",
    "        try:\n",
    "            return [\n",
    "                {\n",
    "                    \"score\": result.score,\n",
    "                    \"text\": result.payload[\"text\"],\n",
    "                    \"title\": result.payload[\"title\"],\n",
    "                    \"url\": result.payload[\"url\"],\n",
    "                    \"metadata\": result.payload[\"metadata\"],\n",
    "                }\n",
    "                for result in self.client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=self.encoder.encode(query).tolist(),\n",
    "                    limit=limit,\n",
    "                )\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching documents: {e}\")\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSCoVRAGPipeline:\n",
    "    def __init__(self, qdrant_url: str = \"http://localhost:6333\"):\n",
    "        self.news_collector = NewsAPICollector()\n",
    "        self.rag_system = QdrantRAGSystem(qdrant_url=qdrant_url)\n",
    "\n",
    "    def collect_and_index_news(self, query: str):\n",
    "        \"\"\"Complete pipeline to collect news and index in RAG system\"\"\"\n",
    "        print(\"Fetching articles from News API...\")\n",
    "        if not (articles := self.news_collector.fetch_articles(query=query)):\n",
    "            print(\"No articles found\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(articles)} articles. Preprocessing...\")\n",
    "        processed_docs = self.news_collector.preprocess_articles(articles)\n",
    "        print(f\"Processed {len(processed_docs)} articles. Adding to Qdrant...\")\n",
    "        self.rag_system.add_documents(processed_docs)\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "\n",
    "    def query_rag_system(self, user_query: str, num_results: int = 3):\n",
    "        \"\"\"Query the RAG system and return relevant information\"\"\"\n",
    "        print(f\"Searching for: {user_query}\")\n",
    "        if not (results := self.rag_system.search_similar(user_query, num_results)):\n",
    "            print(\"No relevant articles found\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nFound {len(results)} relevant articles:\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {result['title']}\")\n",
    "            print(f\"   Relevance Score: {result['score']:.3f}\")\n",
    "            print(f\"   Source: {result['metadata']['source_name']}\")\n",
    "            print(f\"   URL: {result['url']}\")\n",
    "            print(f\"   Preview: {result['text'][:200]}...\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = SARSCoVRAGPipeline()\n",
    "\n",
    "for query in [\"covid\", \"sars\", \"flu\", \"influenza\", \"infectious\", \"viral\"]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    pipeline.collect_and_index_news(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.query_rag_system(\"What are the latest variants of SARS-CoV-2?\", num_results=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-indicium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
