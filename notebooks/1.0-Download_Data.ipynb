{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sqlite3\n",
    "from typing import Callable\n",
    "\n",
    "from camelot import read_pdf\n",
    "from pandas import (\n",
    "    Categorical,\n",
    "    DataFrame,\n",
    "    Series,\n",
    "    concat,\n",
    "    isna,\n",
    "    read_csv,\n",
    "    set_option,\n",
    "    to_datetime,\n",
    "    to_numeric,\n",
    ")\n",
    "\n",
    "set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo Schema de dados do dicionario de dados\n",
    "\n",
    "Para este caso em especifico eu não fiz nenhuma estrutura de processamento de texto por que o intuito era extrair a informação de forma rapida.\n",
    "\n",
    "Poderia-se criar uma pipeline de extração de conteudo cru com uma lib como `pdfplumber` extraindo texto bruto e iterando sobre coletando nome, tipo e info das colunas + metadados opcionais, mas optei por agilidade na solução. Isso aqui teoricamente é pra ser feito uma vez só, então não gastei muitos recursos encima, como faria numa solução real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_markdown_table(df: DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a clean markdown table format\n",
    "    \"\"\"\n",
    "    df = df.dropna(axis=1, how=\"all\").fillna(\"\")\n",
    "\n",
    "    # Clean data by removing excessive whitespace\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return tabulate(df, headers=\"keys\", tablefmt=\"pipe\", showindex=False)\n",
    "\n",
    "\n",
    "def is_continuation_row(row: Series) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a row appears to be a continuation from a previous page.\n",
    "\n",
    "    A continuation row is identified when:\n",
    "    - Columns 0-2 are empty/null\n",
    "    - Columns 3-5 contain data\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A pandas Series representing a single row from a dataframe\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the row appears to be a continuation row, False otherwise\n",
    "\n",
    "    Note:\n",
    "        Requires at least 5 columns in the row to perform the check.\n",
    "        Returns False if the row has fewer than 5 columns.\n",
    "    \"\"\"\n",
    "    if len(row) < 5:\n",
    "        return False\n",
    "\n",
    "    return all(  # Check if first 3 columns are empty\n",
    "        isna(row.iloc[j]) or str(row.iloc[j]).strip() == \"\" for j in range(3)\n",
    "    ) and any(  # Check if columns 3-5 have data\n",
    "        not isna(row.iloc[j]) and str(row.iloc[j]).strip() != \"\" for j in range(2, 5)\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_continuation_row(target_df: DataFrame, continuation_row: Series) -> None:\n",
    "    \"\"\"\n",
    "    Merge a continuation row with the last row of the target dataframe.\n",
    "\n",
    "    The function modifies the target dataframe in-place by:\n",
    "    - Filling empty cells in the last row with data from the continuation row\n",
    "    - Concatenating content when both cells contain data\n",
    "\n",
    "    Args:\n",
    "        target_df (pd.DataFrame): The dataframe whose last row will be updated\n",
    "        continuation_row (pd.Series): The row containing continuation data\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies target_df in-place\n",
    "\n",
    "    Note:\n",
    "        If target_df is empty, the function returns without making changes.\n",
    "        Only processes columns that exist in both the target dataframe and continuation row.\n",
    "    \"\"\"\n",
    "    if len(target_df) == 0:\n",
    "        return\n",
    "\n",
    "    last_row_idx = target_df.index[-1]\n",
    "\n",
    "    for col_idx in range(len(continuation_row)):\n",
    "        if col_idx >= len(target_df.columns):\n",
    "            break\n",
    "\n",
    "        col_name = target_df.columns[col_idx]\n",
    "        continuation_value = continuation_row.iloc[col_idx]\n",
    "\n",
    "        # Skip if continuation cell is empty\n",
    "        if pd.isna(continuation_value) or str(continuation_value).strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        existing_value = target_df.loc[last_row_idx, col_name]\n",
    "\n",
    "        # If target cell is empty, use continuation value\n",
    "        if pd.isna(existing_value) or str(existing_value).strip() == \"\":\n",
    "            target_df.loc[last_row_idx, col_name] = continuation_value\n",
    "        else:\n",
    "            # Both have content, concatenate with space\n",
    "            existing = str(existing_value).strip()\n",
    "            new_content = str(continuation_value).strip()\n",
    "            target_df.loc[last_row_idx, col_name] = f\"{existing} {new_content}\"\n",
    "\n",
    "\n",
    "def process_single_dataframe(df: DataFrame, previous_df: DataFrame | None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process a single dataframe, checking for continuation rows and handling merging.\n",
    "\n",
    "    If the first row of the dataframe is identified as a continuation row and there's\n",
    "    a previous dataframe available, the continuation row is merged with the last row\n",
    "    of the previous dataframe and removed from the current dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to process\n",
    "        previous_df (Optional[pd.DataFrame]): The previous dataframe for potential merging.\n",
    "                                            None if this is the first dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the processed dataframe with continuation rows removed\n",
    "                     if they were merged with the previous dataframe\n",
    "\n",
    "    Note:\n",
    "        If df is empty, returns an empty copy.\n",
    "        The previous_df is modified in-place if a merge occurs.\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return df.copy()\n",
    "\n",
    "    current_df = df.copy()\n",
    "    first_row = current_df.iloc[0]\n",
    "\n",
    "    # Check if first row is a continuation row and we have a previous dataframe\n",
    "    if is_continuation_row(first_row) and previous_df is not None:\n",
    "        # Merge with previous dataframe\n",
    "        merge_continuation_row(previous_df, first_row)\n",
    "        # Remove the merged row from current dataframe\n",
    "        current_df = current_df.iloc[1:].copy()\n",
    "\n",
    "    return DataFrame(current_df)\n",
    "\n",
    "\n",
    "def merge_split_rows(dfs: list[DataFrame]) -> list[DataFrame]:\n",
    "    \"\"\"\n",
    "    Merge rows that are split across PDF pages in a list of dataframes.\n",
    "\n",
    "    This function processes a list of dataframes (typically representing tables from\n",
    "    different PDF pages) and merges rows that were split across page boundaries.\n",
    "    Split rows are identified using the continuation row detection logic.\n",
    "\n",
    "    Args:\n",
    "        dfs (List[pd.DataFrame]): List of dataframes to process, typically one per PDF page\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: List of processed dataframes with split rows merged.\n",
    "                           Empty dataframes are excluded from the result.\n",
    "\n",
    "    Note:\n",
    "        The function processes dataframes sequentially, so continuation rows are only\n",
    "        merged with the immediately preceding dataframe in the list.\n",
    "        Original dataframes are not modified; copies are created for processing.\n",
    "\n",
    "    Example:\n",
    "        >>> dfs = [df_page1, df_page2, df_page3]\n",
    "        >>> merged_dfs = merge_split_rows(dfs)\n",
    "        >>> final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    \"\"\"\n",
    "    if not dfs:\n",
    "        return []\n",
    "\n",
    "    processed_dfs: list[DataFrame] = []\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        processed_df = process_single_dataframe(\n",
    "            df, processed_dfs[-1] if processed_dfs else None\n",
    "        )\n",
    "        if len(processed_df) > 0:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "    return processed_dfs\n",
    "\n",
    "\n",
    "def clean_markdown_text(text):\n",
    "    \"\"\"\n",
    "    Clean and enhance the markdown text for better LLM processing\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # Trim whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip completely empty lines except single newlines\n",
    "        if line != \"\" or (cleaned_lines and cleaned_lines[-1] != \"\"):\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted markdown tables from the PDF.\n",
      "Markdown conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    content = read_pdf(\n",
    "        \"../docs/dicionario-de-dados-2019-a-2025.pdf\", pages=\"all\", line_scale=40\n",
    "    )\n",
    "    for df in (dfs := [table.df for table in content][1:]):\n",
    "        df.columns = dfs[0].iloc[0]  # Set the first row as column names\n",
    "\n",
    "    dfs[0].drop(index=0, inplace=True)\n",
    "    df_list = [\n",
    "        \"## Concatenated Table\\n\\n\"\n",
    "        + df_to_markdown_table(\n",
    "            all_df := concat(merge_split_rows(dfs), ignore_index=True)\n",
    "        )\n",
    "        + \"\\n\"\n",
    "    ]\n",
    "    all_df[\"Tipo\"] = all_df[\"Tipo\"].str.replace(\"\\n\", \"\")\n",
    "    all_df.to_csv(\"../docs/data_dict.csv\", index=False)\n",
    "    full_markdown = \"\\n\".join(df_list)\n",
    "    print(\"Successfully extracted markdown tables from the PDF.\")\n",
    "    cleaned_markdown = clean_markdown_text(full_markdown)  # .split(\"\\n\")\n",
    "    print(\"Markdown conversion completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo bases de dados e dividindo entre ambientes\n",
    "- Esse processo aqui eu jogaria esses Data Lake pra ser acessivel ao time.\n",
    "- Vamos fingir q o diretorio data é nosso Lake.\n",
    "- Com um sqlite tambem simulando um postgres, se sobrar tempo eu troco por um Postgres num docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_type_mapping(data_dict_df: DataFrame) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Enhanced type mapping with better pattern matching and validation.\n",
    "    \"\"\"\n",
    "    type_mapping = {}\n",
    "\n",
    "    for _, row in data_dict_df.iterrows():\n",
    "        col_name = row[\"DBF\"]\n",
    "        col_type = row[\"Tipo\"]\n",
    "\n",
    "        if isna(col_name) or isna(col_type):  # type: ignore\n",
    "            continue\n",
    "\n",
    "        col_type_clean = str(col_type).strip().replace(\" \", \"\")\n",
    "\n",
    "        # Enhanced pattern matching\n",
    "        if re.search(r\"Date|Data\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"datetime64[ns]\"\n",
    "        elif re.search(r\"Number\\(\\d+\\)\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"float64\"\n",
    "        elif re.search(r\"Varchar2?\\(1\\)\", col_type_clean, re.IGNORECASE):\n",
    "            # Single character fields are good candidates for categories\n",
    "            type_mapping[col_name] = \"category\"\n",
    "        elif re.search(r\"Varchar2?\\(\\d+\\)\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"string\"\n",
    "        else:  # Default fallback for malformed or unrecognized types\n",
    "            type_mapping[col_name] = \"string\"\n",
    "\n",
    "    return type_mapping\n",
    "\n",
    "\n",
    "def create_converters(dtype_mapping: dict[str, str]) -> dict[str, Callable]:\n",
    "    \"\"\"\n",
    "    Create converter functions with proper scope handling.\n",
    "    \"\"\"\n",
    "    converters = {}\n",
    "\n",
    "    for col_name, dtype in dtype_mapping.items():\n",
    "        if dtype == \"datetime64[ns]\":\n",
    "            converters[col_name] = lambda x: to_datetime(\n",
    "                x, format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "            )\n",
    "        elif dtype == \"float64\":\n",
    "            converters[col_name] = lambda x: to_numeric(x, errors=\"coerce\")\n",
    "        elif dtype == \"category\":\n",
    "            converters[col_name] = lambda x: Categorical(x)\n",
    "        elif dtype == \"string\":\n",
    "            converters[col_name] = lambda x: x.astype(\"string\")\n",
    "\n",
    "    return converters\n",
    "\n",
    "\n",
    "def apply_dtype_conversions(df: DataFrame, dtype_mapping: dict[str, str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply data type conversions to DataFrame.\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "\n",
    "    for col_name, target_dtype in dtype_mapping.items():\n",
    "        if col_name not in df_converted.columns:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if target_dtype == \"datetime64[ns]\":\n",
    "                df_converted[col_name] = to_datetime(\n",
    "                    df_converted[col_name], format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "                )\n",
    "            elif target_dtype == \"float64\":\n",
    "                df_converted[col_name] = to_numeric(\n",
    "                    df_converted[col_name], errors=\"coerce\"\n",
    "                )\n",
    "            elif target_dtype == \"category\":\n",
    "                df_converted[col_name] = df_converted[col_name].astype(\"category\")\n",
    "            elif target_dtype == \"string\":\n",
    "                df_converted[col_name] = df_converted[col_name].astype(\"string\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: Could not convert column {col_name} to {target_dtype}: {e}\"\n",
    "            )\n",
    "\n",
    "    return df_converted\n",
    "\n",
    "\n",
    "def optimize_for_target_format(df: DataFrame, target_format: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply format-specific optimizations.\n",
    "    \"\"\"\n",
    "    df_optimized = df.copy()\n",
    "\n",
    "    if target_format.lower() == \"parquet\":\n",
    "        # Parquet handles categories efficiently\n",
    "        for col in df_optimized.select_dtypes(include=[\"object\"]).columns:\n",
    "            if df_optimized[col].nunique() / len(df_optimized) < 0.5:  # < 50% unique\n",
    "                df_optimized[col] = df_optimized[col].astype(\"category\")\n",
    "\n",
    "    elif target_format.lower() == \"sqlite\":\n",
    "        # SQLite doesn't support pandas categories\n",
    "        for col in df_optimized.select_dtypes(include=[\"category\"]).columns:\n",
    "            df_optimized[col] = df_optimized[col].astype(\"string\")\n",
    "\n",
    "    return df_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading year 2019 dataset\n",
      "Reading year 2020 dataset\n",
      "Reading year 2021 dataset\n",
      "Reading year 2022 dataset\n",
      "Reading year 2023 dataset\n",
      "Reading year 2024 dataset\n"
     ]
    }
   ],
   "source": [
    "dtype_mapping = get_column_type_mapping(all_df)\n",
    "\n",
    "DFS: list[DataFrame] = []\n",
    "for y in range(19, 25):\n",
    "    print(f\"Reading year 20{y} dataset\")\n",
    "    DFS.append(\n",
    "        apply_dtype_conversions(\n",
    "            read_csv(\n",
    "                f\"https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SRAG/20{y}/INFLUD{y}-26-06-2025.csv\",\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            ),\n",
    "            dtype_mapping,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n"
     ]
    }
   ],
   "source": [
    "for df, year in zip(DFS, range(19, 25)):\n",
    "    # Save parquet with optimized compression\n",
    "    print(f\"Saving year 20{year} dataset\")\n",
    "    df.to_csv(f\"../data/raw/srag_20{year}.csv\", index=False, date_format=\"%Y-%m-%d\")\n",
    "    optimize_for_target_format(df, \"parquet\").to_parquet(\n",
    "        f\"../data/raw/srag_20{year}.parquet\", index=False, compression=\"snappy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging raw data into one database\")\n",
    "merged_df = read_csv(\n",
    "    \"../data/interim/srag_2019_2024.csv\"\n",
    ")  # concat(DFS, ignore_index=True)\n",
    "merged_df.to_csv(\n",
    "    \"../data/interim/srag_2019_2024.csv\", index=False, date_format=\"%Y-%m-%d\"\n",
    ")\n",
    "# merged_df.to_parquet(  # Não otimizando pq não limpei a base\n",
    "#     \"../data/interim/srag_2019_2024.parquet\", index=False, compression=\"snappy\"\n",
    "# )\n",
    "conn = sqlite3.connect(\"../data/interim/srag_2019_2024.db\")\n",
    "optimize_for_target_format(\n",
    "    merged_df, \"sqlite\"\n",
    ").to_sql(  # Save to SQLite with proper type conversion\n",
    "    \"srag_cases\",\n",
    "    conn,\n",
    "    if_exists=\"replace\",\n",
    "    index=False,\n",
    "    dtype={\n",
    "        col: \"TIMESTAMP\" if dtype == \"datetime64[ns]\" else None\n",
    "        for col, dtype in merged_df.dtypes.items()\n",
    "    },\n",
    ")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NU_NOTIFIC',\n",
       " 'DT_NOTIFIC',\n",
       " 'SEM_NOT',\n",
       " 'DT_SIN_PRI',\n",
       " 'SEM_PRI',\n",
       " 'SG_UF_NOT',\n",
       " 'ID_REGIONA',\n",
       " 'CO_REGIONA',\n",
       " 'ID_MUNICIP',\n",
       " 'CO_MUN_NOT',\n",
       " 'CS_SEXO',\n",
       " 'DT_NASC',\n",
       " 'NU_IDADE_N',\n",
       " 'TP_IDADE',\n",
       " 'COD_IDADE',\n",
       " 'CS_GESTANT',\n",
       " 'CS_RACA',\n",
       " 'CS_ETINIA',\n",
       " 'CS_ESCOL_N',\n",
       " 'ID_PAIS',\n",
       " 'CO_PAIS',\n",
       " 'SG_UF',\n",
       " 'ID_RG_RESI',\n",
       " 'CO_RG_RESI',\n",
       " 'ID_MN_RESI',\n",
       " 'CO_MUN_RES',\n",
       " 'CS_ZONA',\n",
       " 'NOSOCOMIAL',\n",
       " 'AVE_SUINO',\n",
       " 'FEBRE',\n",
       " 'TOSSE',\n",
       " 'GARGANTA',\n",
       " 'DISPNEIA',\n",
       " 'DESC_RESP',\n",
       " 'SATURACAO',\n",
       " 'DIARREIA',\n",
       " 'VOMITO',\n",
       " 'OUTRO_SIN',\n",
       " 'OUTRO_DES',\n",
       " 'FATOR_RISC',\n",
       " 'PUERPERA',\n",
       " 'CARDIOPATI',\n",
       " 'HEMATOLOGI',\n",
       " 'SIND_DOWN',\n",
       " 'HEPATICA',\n",
       " 'ASMA',\n",
       " 'DIABETES',\n",
       " 'NEUROLOGIC',\n",
       " 'PNEUMOPATI',\n",
       " 'IMUNODEPRE',\n",
       " 'RENAL',\n",
       " 'OBESIDADE',\n",
       " 'OBES_IMC',\n",
       " 'OUT_MORBI',\n",
       " 'MORB_DESC',\n",
       " 'TABAG',\n",
       " 'VACINA',\n",
       " 'DT_UT_DOSE',\n",
       " 'MAE_VAC',\n",
       " 'DT_VAC_MAE',\n",
       " 'M_AMAMENTA',\n",
       " 'DT_DOSEUNI',\n",
       " 'DT_1_DOSE',\n",
       " 'DT_2_DOSE',\n",
       " 'ANTIVIRAL',\n",
       " 'TP_ANTIVIR',\n",
       " 'OUT_ANTIV',\n",
       " 'DT_ANTIVIR',\n",
       " 'HOSPITAL',\n",
       " 'DT_INTERNA',\n",
       " 'SG_UF_INTE',\n",
       " 'ID_RG_INTE',\n",
       " 'CO_RG_INTE',\n",
       " 'ID_MN_INTE',\n",
       " 'CO_MU_INTE',\n",
       " 'NM_UN_INTE',\n",
       " 'UTI',\n",
       " 'DT_ENTUTI',\n",
       " 'DT_SAIDUTI',\n",
       " 'SUPORT_VEN',\n",
       " 'RAIOX_RES',\n",
       " 'RAIOX_OUT',\n",
       " 'DT_RAIOX',\n",
       " 'AMOSTRA',\n",
       " 'DT_COLETA',\n",
       " 'TP_AMOSTRA',\n",
       " 'OUT_AMOST',\n",
       " 'PCR_RESUL',\n",
       " 'DT_PCR',\n",
       " 'POS_PCRFLU',\n",
       " 'TP_FLU_PCR',\n",
       " 'PCR_FLUASU',\n",
       " 'FLUASU_OUT',\n",
       " 'PCR_FLUBLI',\n",
       " 'FLUBLI_OUT',\n",
       " 'POS_PCROUT',\n",
       " 'PCR_VSR',\n",
       " 'PCR_PARA1',\n",
       " 'PCR_PARA2',\n",
       " 'PCR_PARA3',\n",
       " 'PCR_PARA4',\n",
       " 'PCR_ADENO',\n",
       " 'PCR_METAP',\n",
       " 'PCR_BOCA',\n",
       " 'PCR_RINO',\n",
       " 'PCR_OUTRO',\n",
       " 'DS_PCR_OUT',\n",
       " 'CLASSI_FIN',\n",
       " 'CLASSI_OUT',\n",
       " 'CRITERIO',\n",
       " 'EVOLUCAO',\n",
       " 'DT_EVOLUCA',\n",
       " 'DT_ENCERRA',\n",
       " 'DT_DIGITA',\n",
       " 'HISTO_VGM',\n",
       " 'PAIS_VGM',\n",
       " 'CO_PS_VGM',\n",
       " 'LO_PS_VGM',\n",
       " 'DT_VGM',\n",
       " 'DT_RT_VGM',\n",
       " 'PCR_SARS2',\n",
       " 'PAC_COCBO',\n",
       " 'PAC_DSCBO',\n",
       " 'OUT_ANIM',\n",
       " 'DOR_ABD',\n",
       " 'FADIGA',\n",
       " 'PERD_OLFT',\n",
       " 'PERD_PALA',\n",
       " 'TOMO_RES',\n",
       " 'TOMO_OUT',\n",
       " 'DT_TOMO',\n",
       " 'TP_TES_AN',\n",
       " 'DT_RES_AN',\n",
       " 'RES_AN',\n",
       " 'POS_AN_FLU',\n",
       " 'TP_FLU_AN',\n",
       " 'POS_AN_OUT',\n",
       " 'AN_SARS2',\n",
       " 'AN_VSR',\n",
       " 'AN_PARA1',\n",
       " 'AN_PARA2',\n",
       " 'AN_PARA3',\n",
       " 'AN_ADENO',\n",
       " 'AN_OUTRO',\n",
       " 'DS_AN_OUT',\n",
       " 'TP_AM_SOR',\n",
       " 'SOR_OUT',\n",
       " 'DT_CO_SOR',\n",
       " 'TP_SOR',\n",
       " 'OUT_SOR',\n",
       " 'DT_RES',\n",
       " 'RES_IGG',\n",
       " 'RES_IGM',\n",
       " 'RES_IGA',\n",
       " 'POV_CT',\n",
       " 'TP_POV_CT',\n",
       " 'TEM_CPF',\n",
       " 'ESTRANG',\n",
       " 'VACINA_COV',\n",
       " 'DOSE_1_COV',\n",
       " 'DOSE_2_COV',\n",
       " 'DOSE_REF',\n",
       " 'DOSE_2REF',\n",
       " 'DOSE_ADIC',\n",
       " 'DOS_RE_BI',\n",
       " 'FAB_COV_1',\n",
       " 'FAB_COV_2',\n",
       " 'FAB_COVRF',\n",
       " 'FAB_COVRF2',\n",
       " 'FAB_ADIC',\n",
       " 'FAB_RE_BI',\n",
       " 'LOTE_1_COV',\n",
       " 'LOTE_2_COV',\n",
       " 'LOTE_REF',\n",
       " 'LOTE_REF2',\n",
       " 'LOTE_ADIC',\n",
       " 'LOT_RE_BI',\n",
       " 'FNT_IN_COV',\n",
       " 'TRAT_COV',\n",
       " 'TIPO_TRAT',\n",
       " 'DT_TRT_COV',\n",
       " 'OUT_TRAT',\n",
       " 'SURTO_SG',\n",
       " 'CO_DETEC',\n",
       " 'VG_OMS',\n",
       " 'VG_OMSOUT',\n",
       " 'VG_LIN',\n",
       " 'VG_MET',\n",
       " 'VG_METOUT',\n",
       " 'VG_DTRES',\n",
       " 'VG_ENC',\n",
       " 'VG_REINF',\n",
       " 'VG_CODEST',\n",
       " 'REINF']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-indicium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
