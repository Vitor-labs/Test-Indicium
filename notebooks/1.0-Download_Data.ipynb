{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "import uuid\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Callable\n",
    "\n",
    "import requests\n",
    "from camelot import read_pdf\n",
    "from dotenv import load_dotenv\n",
    "from pandas import (\n",
    "    Categorical,\n",
    "    DataFrame,\n",
    "    Series,\n",
    "    concat,\n",
    "    isna,\n",
    "    read_csv,\n",
    "    set_option,\n",
    "    to_datetime,\n",
    "    to_numeric,\n",
    ")\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.models import Distance, PointStruct, VectorParams\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "load_dotenv()  # i don't use dotenv, but in case you didn't config vscode previously\n",
    "\n",
    "set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraindo Schema de dados do dicionario de dados\n",
    "\n",
    "Para este caso em especifico eu não fiz nenhuma estrutura de processamento de texto por que o intuito era extrair a informação de forma rapida.\n",
    "\n",
    "Poderia-se criar uma pipeline de extração de conteudo cru com uma lib como `pdfplumber` extraindo texto bruto e iterando sobre coletando nome, tipo e info das colunas + metadados opcionais, mas optei por agilidade na solução. Isso aqui teoricamente é pra ser feito uma vez só, então não gastei muitos recursos encima, como faria numa solução real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_markdown_table(df: DataFrame) -> str:\n",
    "    \"\"\"\n",
    "    Convert a pandas DataFrame to a clean markdown table format\n",
    "    \"\"\"\n",
    "    df = df.dropna(axis=1, how=\"all\").fillna(\"\")\n",
    "\n",
    "    # Clean data by removing excessive whitespace\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"object\":\n",
    "            df[col] = df[col].astype(str).str.strip()\n",
    "\n",
    "    return tabulate(df, headers=\"keys\", tablefmt=\"pipe\", showindex=False)\n",
    "\n",
    "\n",
    "def is_continuation_row(row: Series) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a row appears to be a continuation from a previous page.\n",
    "\n",
    "    A continuation row is identified when:\n",
    "    - Columns 0-2 are empty/null\n",
    "    - Columns 3-5 contain data\n",
    "\n",
    "    Args:\n",
    "        row (pd.Series): A pandas Series representing a single row from a dataframe\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the row appears to be a continuation row, False otherwise\n",
    "\n",
    "    Note:\n",
    "        Requires at least 5 columns in the row to perform the check.\n",
    "        Returns False if the row has fewer than 5 columns.\n",
    "    \"\"\"\n",
    "    if len(row) < 5:\n",
    "        return False\n",
    "\n",
    "    return all(  # Check if first 3 columns are empty\n",
    "        isna(row.iloc[j]) or str(row.iloc[j]).strip() == \"\" for j in range(3)\n",
    "    ) and any(  # Check if columns 3-5 have data\n",
    "        not isna(row.iloc[j]) and str(row.iloc[j]).strip() != \"\" for j in range(2, 5)\n",
    "    )\n",
    "\n",
    "\n",
    "def merge_continuation_row(target_df: DataFrame, continuation_row: Series) -> None:\n",
    "    \"\"\"\n",
    "    Merge a continuation row with the last row of the target dataframe.\n",
    "\n",
    "    The function modifies the target dataframe in-place by:\n",
    "    - Filling empty cells in the last row with data from the continuation row\n",
    "    - Concatenating content when both cells contain data\n",
    "\n",
    "    Args:\n",
    "        target_df (pd.DataFrame): The dataframe whose last row will be updated\n",
    "        continuation_row (pd.Series): The row containing continuation data\n",
    "\n",
    "    Returns:\n",
    "        None: Modifies target_df in-place\n",
    "\n",
    "    Note:\n",
    "        If target_df is empty, the function returns without making changes.\n",
    "        Only processes columns that exist in both the target dataframe and continuation row.\n",
    "    \"\"\"\n",
    "    if len(target_df) == 0:\n",
    "        return\n",
    "\n",
    "    last_row_idx = target_df.index[-1]\n",
    "\n",
    "    for col_idx in range(len(continuation_row)):\n",
    "        if col_idx >= len(target_df.columns):\n",
    "            break\n",
    "\n",
    "        col_name = target_df.columns[col_idx]\n",
    "        continuation_value = continuation_row.iloc[col_idx]\n",
    "\n",
    "        # Skip if continuation cell is empty\n",
    "        if pd.isna(continuation_value) or str(continuation_value).strip() == \"\":\n",
    "            continue\n",
    "\n",
    "        existing_value = target_df.loc[last_row_idx, col_name]\n",
    "\n",
    "        # If target cell is empty, use continuation value\n",
    "        if pd.isna(existing_value) or str(existing_value).strip() == \"\":\n",
    "            target_df.loc[last_row_idx, col_name] = continuation_value\n",
    "        else:\n",
    "            # Both have content, concatenate with space\n",
    "            existing = str(existing_value).strip()\n",
    "            new_content = str(continuation_value).strip()\n",
    "            target_df.loc[last_row_idx, col_name] = f\"{existing} {new_content}\"\n",
    "\n",
    "\n",
    "def process_single_dataframe(df: DataFrame, previous_df: DataFrame | None) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Process a single dataframe, checking for continuation rows and handling merging.\n",
    "\n",
    "    If the first row of the dataframe is identified as a continuation row and there's\n",
    "    a previous dataframe available, the continuation row is merged with the last row\n",
    "    of the previous dataframe and removed from the current dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe to process\n",
    "        previous_df (Optional[pd.DataFrame]): The previous dataframe for potential merging.\n",
    "                                            None if this is the first dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A copy of the processed dataframe with continuation rows removed\n",
    "                     if they were merged with the previous dataframe\n",
    "\n",
    "    Note:\n",
    "        If df is empty, returns an empty copy.\n",
    "        The previous_df is modified in-place if a merge occurs.\n",
    "    \"\"\"\n",
    "    if len(df) == 0:\n",
    "        return df.copy()\n",
    "\n",
    "    current_df = df.copy()\n",
    "    first_row = current_df.iloc[0]\n",
    "\n",
    "    # Check if first row is a continuation row and we have a previous dataframe\n",
    "    if is_continuation_row(first_row) and previous_df is not None:\n",
    "        # Merge with previous dataframe\n",
    "        merge_continuation_row(previous_df, first_row)\n",
    "        # Remove the merged row from current dataframe\n",
    "        current_df = current_df.iloc[1:].copy()\n",
    "\n",
    "    return DataFrame(current_df)\n",
    "\n",
    "\n",
    "def merge_split_rows(dfs: list[DataFrame]) -> list[DataFrame]:\n",
    "    \"\"\"\n",
    "    Merge rows that are split across PDF pages in a list of dataframes.\n",
    "\n",
    "    This function processes a list of dataframes (typically representing tables from\n",
    "    different PDF pages) and merges rows that were split across page boundaries.\n",
    "    Split rows are identified using the continuation row detection logic.\n",
    "\n",
    "    Args:\n",
    "        dfs (List[pd.DataFrame]): List of dataframes to process, typically one per PDF page\n",
    "\n",
    "    Returns:\n",
    "        List[pd.DataFrame]: List of processed dataframes with split rows merged.\n",
    "                           Empty dataframes are excluded from the result.\n",
    "\n",
    "    Note:\n",
    "        The function processes dataframes sequentially, so continuation rows are only\n",
    "        merged with the immediately preceding dataframe in the list.\n",
    "        Original dataframes are not modified; copies are created for processing.\n",
    "\n",
    "    Example:\n",
    "        >>> dfs = [df_page1, df_page2, df_page3]\n",
    "        >>> merged_dfs = merge_split_rows(dfs)\n",
    "        >>> final_df = pd.concat(merged_dfs, ignore_index=True)\n",
    "    \"\"\"\n",
    "    if not dfs:\n",
    "        return []\n",
    "\n",
    "    processed_dfs: list[DataFrame] = []\n",
    "\n",
    "    for i, df in enumerate(dfs):\n",
    "        processed_df = process_single_dataframe(\n",
    "            df, processed_dfs[-1] if processed_dfs else None\n",
    "        )\n",
    "        if len(processed_df) > 0:\n",
    "            processed_dfs.append(processed_df)\n",
    "\n",
    "    return processed_dfs\n",
    "\n",
    "\n",
    "def clean_markdown_text(text):\n",
    "    \"\"\"\n",
    "    Clean and enhance the markdown text for better LLM processing\n",
    "    \"\"\"\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in text.split(\"\\n\"):\n",
    "        # Trim whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # Skip completely empty lines except single newlines\n",
    "        if line != \"\" or (cleaned_lines and cleaned_lines[-1] != \"\"):\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully extracted markdown tables from the PDF.\n",
      "Markdown conversion completed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    content = read_pdf(\n",
    "        \"../docs/dicionario-de-dados-2019-a-2025.pdf\", pages=\"all\", line_scale=40\n",
    "    )\n",
    "    for df in (dfs := [table.df for table in content][1:]):\n",
    "        df.columns = dfs[0].iloc[0]  # Set the first row as column names\n",
    "\n",
    "    dfs[0].drop(index=0, inplace=True)\n",
    "    df_list = [\n",
    "        \"## Concatenated Table\\n\\n\"\n",
    "        + df_to_markdown_table(\n",
    "            all_df := concat(merge_split_rows(dfs), ignore_index=True)\n",
    "        )\n",
    "        + \"\\n\"\n",
    "    ]\n",
    "    all_df[\"Tipo\"] = all_df[\"Tipo\"].str.replace(\"\\n\", \"\")\n",
    "    all_df.to_csv(\"../docs/data_dict.csv\", index=False)\n",
    "    full_markdown = \"\\n\".join(df_list)\n",
    "    print(\"Successfully extracted markdown tables from the PDF.\")\n",
    "    cleaned_markdown = clean_markdown_text(full_markdown)  # .split(\"\\n\")\n",
    "    print(\"Markdown conversion completed successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error extracting tables: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraindo bases de dados e dividindo entre ambientes\n",
    "- Esse processo aqui eu jogaria esses Data Lake pra ser acessivel ao time.\n",
    "- Vamos fingir q o diretorio data é nosso Lake.\n",
    "- Com um sqlite tambem simulando um postgres, se sobrar tempo eu troco por um Postgres num docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_column_type_mapping(data_dict_df: DataFrame) -> dict[str, str]:\n",
    "    \"\"\"\n",
    "    Enhanced type mapping with better pattern matching and validation.\n",
    "    \"\"\"\n",
    "    type_mapping = {}\n",
    "\n",
    "    for _, row in data_dict_df.iterrows():\n",
    "        col_name = row[\"DBF\"]\n",
    "        col_type = row[\"Tipo\"]\n",
    "\n",
    "        if isna(col_name) or isna(col_type):  # type: ignore\n",
    "            continue\n",
    "\n",
    "        col_type_clean = str(col_type).strip().replace(\" \", \"\")\n",
    "\n",
    "        # Enhanced pattern matching\n",
    "        if re.search(r\"Date|Data\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"datetime64[ns]\"\n",
    "        elif re.search(r\"Number\\(\\d+\\)\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"float64\"\n",
    "        elif re.search(r\"Varchar2?\\(1\\)\", col_type_clean, re.IGNORECASE):\n",
    "            # Single character fields are good candidates for categories\n",
    "            type_mapping[col_name] = \"category\"\n",
    "        elif re.search(r\"Varchar2?\\(\\d+\\)\", col_type_clean, re.IGNORECASE):\n",
    "            type_mapping[col_name] = \"string\"\n",
    "        else:  # Default fallback for malformed or unrecognized types\n",
    "            type_mapping[col_name] = \"string\"\n",
    "\n",
    "    return type_mapping\n",
    "\n",
    "\n",
    "def create_converters(dtype_mapping: dict[str, str]) -> dict[str, Callable]:\n",
    "    \"\"\"\n",
    "    Create converter functions with proper scope handling.\n",
    "    \"\"\"\n",
    "    converters = {}\n",
    "\n",
    "    for col_name, dtype in dtype_mapping.items():\n",
    "        if dtype == \"datetime64[ns]\":\n",
    "            converters[col_name] = lambda x: to_datetime(\n",
    "                x, format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "            )\n",
    "        elif dtype == \"float64\":\n",
    "            converters[col_name] = lambda x: to_numeric(x, errors=\"coerce\")\n",
    "        elif dtype == \"category\":\n",
    "            converters[col_name] = lambda x: Categorical(x)\n",
    "        elif dtype == \"string\":\n",
    "            converters[col_name] = lambda x: x.astype(\"string\")\n",
    "\n",
    "    return converters\n",
    "\n",
    "\n",
    "def apply_dtype_conversions(df: DataFrame, dtype_mapping: dict[str, str]) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply data type conversions to DataFrame.\n",
    "    \"\"\"\n",
    "    df_converted = df.copy()\n",
    "\n",
    "    for col_name, target_dtype in dtype_mapping.items():\n",
    "        if col_name not in df_converted.columns:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            if target_dtype == \"datetime64[ns]\":\n",
    "                df_converted[col_name] = to_datetime(\n",
    "                    df_converted[col_name], format=\"%d/%m/%Y\", errors=\"coerce\"\n",
    "                )\n",
    "            elif target_dtype == \"float64\":\n",
    "                df_converted[col_name] = to_numeric(\n",
    "                    df_converted[col_name], errors=\"coerce\"\n",
    "                )\n",
    "            elif target_dtype == \"category\":\n",
    "                df_converted[col_name] = df_converted[col_name].astype(\"category\")\n",
    "            elif target_dtype == \"string\":\n",
    "                df_converted[col_name] = df_converted[col_name].astype(\"string\")\n",
    "        except Exception as e:\n",
    "            print(\n",
    "                f\"Warning: Could not convert column {col_name} to {target_dtype}: {e}\"\n",
    "            )\n",
    "\n",
    "    return df_converted\n",
    "\n",
    "\n",
    "def optimize_for_target_format(df: DataFrame, target_format: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Apply format-specific optimizations.\n",
    "    \"\"\"\n",
    "    df_optimized = df.copy()\n",
    "\n",
    "    if target_format.lower() == \"parquet\":\n",
    "        # Parquet handles categories efficiently\n",
    "        for col in df_optimized.select_dtypes(include=[\"object\"]).columns:\n",
    "            if df_optimized[col].nunique() / len(df_optimized) < 0.5:  # < 50% unique\n",
    "                df_optimized[col] = df_optimized[col].astype(\"category\")\n",
    "\n",
    "    elif target_format.lower() == \"sqlite\":\n",
    "        # SQLite doesn't support pandas categories\n",
    "        for col in df_optimized.select_dtypes(include=[\"category\"]).columns:\n",
    "            df_optimized[col] = df_optimized[col].astype(\"string\")\n",
    "\n",
    "    return df_optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading year 2019 dataset\n",
      "Reading year 2020 dataset\n",
      "Reading year 2021 dataset\n",
      "Reading year 2022 dataset\n",
      "Reading year 2023 dataset\n",
      "Reading year 2024 dataset\n"
     ]
    }
   ],
   "source": [
    "dtype_mapping = get_column_type_mapping(all_df)\n",
    "\n",
    "DFS: list[DataFrame] = []\n",
    "for y in range(19, 25):\n",
    "    print(f\"Reading year 20{y} dataset\")\n",
    "    DFS.append(\n",
    "        apply_dtype_conversions(\n",
    "            read_csv(\n",
    "                f\"https://s3.sa-east-1.amazonaws.com/ckan.saude.gov.br/SRAG/20{y}/INFLUD{y}-26-06-2025.csv\",\n",
    "                sep=\";\",\n",
    "                encoding=\"latin1\",\n",
    "                low_memory=False,\n",
    "            ),\n",
    "            dtype_mapping,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n",
      "Saving year 2024 dataset\n"
     ]
    }
   ],
   "source": [
    "for df, year in zip(DFS, range(19, 25)):\n",
    "    # Save parquet with optimized compression\n",
    "    print(f\"Saving year 20{year} dataset\")\n",
    "    df.to_csv(f\"../data/raw/srag_20{year}.csv\", index=False, date_format=\"%Y-%m-%d\")\n",
    "    optimize_for_target_format(df, \"parquet\").to_parquet(\n",
    "        f\"../data/raw/srag_20{year}.parquet\", index=False, compression=\"snappy\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Merging raw data into one database\")\n",
    "merged_df = concat(DFS, ignore_index=True)\n",
    "merged_df.to_csv(\n",
    "    \"../data/interim/srag_2019_2024.csv\", index=False, date_format=\"%Y-%m-%d\"\n",
    ")\n",
    "merged_df.to_parquet(  # Não otimizando pq não limpei a base\n",
    "    \"../data/interim/srag_2019_2024.parquet\", index=False, compression=\"snappy\"\n",
    ")\n",
    "with sqlite3.connect(\"../data/interim/srag_2019_2024.db\") as conn:\n",
    "    optimize_for_target_format(\n",
    "        merged_df, \"sqlite\"\n",
    "    ).to_sql(  # Save to SQLite with proper type conversion\n",
    "        \"srag_cases\",\n",
    "        conn,\n",
    "        if_exists=\"replace\",\n",
    "        index=False,\n",
    "        dtype={\n",
    "            col: \"TIMESTAMP\" if dtype == \"datetime64[ns]\" else None\n",
    "            for col, dtype in merged_df.dtypes.items()\n",
    "        },\n",
    "    )\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usando NEWS API e salvando embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsAPICollector:\n",
    "    def __init__(self):\n",
    "        self.BASE_URL = \"https://newsapi.org/v1/everything\"\n",
    "        self.API_KEY = os.getenv(\"NEWS_API_KEY\")  # `apiKey` field\n",
    "\n",
    "    def fetch_articles(\n",
    "        self,\n",
    "        query: str,\n",
    "        language: str = \"en\",\n",
    "        sort_by: str = \"relevancy\",  # Use 'relevancy' instead of 'popularity'\n",
    "        page_size: int = 100,\n",
    "        days_back: int = 29,\n",
    "    ) -> list[dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Fetch articles from News API for a specific query\n",
    "        \"\"\"\n",
    "        to_date = datetime.now()\n",
    "        from_date = to_date - timedelta(days=min(days_back, 29))  # Ensure max 29 days\n",
    "\n",
    "        try:\n",
    "            response = requests.get(\n",
    "                self.BASE_URL,\n",
    "                params={\n",
    "                    \"q\": query,\n",
    "                    \"language\": language,\n",
    "                    \"sortBy\": sort_by,  # 'relevancy', 'popularity', or 'publishedAt'\n",
    "                    \"pageSize\": min(page_size, 100),  # Max 100 per request\n",
    "                    \"from\": from_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"to\": to_date.strftime(\"%Y-%m-%d\"),\n",
    "                    \"apiKey\": self.API_KEY,\n",
    "                },\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "\n",
    "            if data[\"status\"] == \"ok\":\n",
    "                return data[\"articles\"]\n",
    "            else:\n",
    "                print(f\"API Error: {data.get('message', 'Unknown error')}\")\n",
    "                return []\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def preprocess_articles(\n",
    "        self, articles: list[dict[str, str]]\n",
    "    ) -> list[dict[str, str | dict[str, str]]]:\n",
    "        \"\"\"\n",
    "        Clean and preprocess articles for RAG system\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"id\": f\"news_{hash(article['url'])}\",\n",
    "                # Combine title, description, and content\n",
    "                \"text\": f\"{article.get('title', '')} {article.get('description', '')} {article.get('content', '')}\".strip(),\n",
    "                \"title\": article.get(\"title\", \"\"),\n",
    "                \"url\": article.get(\"url\", \"\"),\n",
    "                \"published_at\": article.get(\"publishedAt\", \"\"),\n",
    "                \"source\": article.get(\"source\", {}).get(\"name\", \"\"),\n",
    "                \"metadata\": {\n",
    "                    \"source_type\": \"news_api\",\n",
    "                    \"topic\": \"sars_cov\",\n",
    "                    \"url\": article.get(\"url\", \"\"),\n",
    "                    \"published_date\": article.get(\"publishedAt\", \"\"),\n",
    "                    \"source_name\": article.get(\"source\", {}).get(\"name\", \"\"),\n",
    "                },\n",
    "            }\n",
    "            for article in articles\n",
    "            if not article.get(\"content\") or article[\"content\"] == \"[Removed]\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QdrantRAGSystem:\n",
    "    def __init__(\n",
    "        self,\n",
    "        qdrant_url: str = \"http://localhost:6333\",\n",
    "        collection_name: str = \"sars_cov_news\",\n",
    "        embedding_model: str = \"all-MiniLM-L6-v2\",\n",
    "    ):\n",
    "        self.client = QdrantClient(url=qdrant_url)\n",
    "        self.collection_name = collection_name\n",
    "        self.encoder = SentenceTransformer(embedding_model)\n",
    "        self.vector_size = self.encoder.get_sentence_embedding_dimension()\n",
    "        self._create_collection()  # Create collection if it doesn't exist\n",
    "\n",
    "    def _create_collection(self):\n",
    "        \"\"\"Create Qdrant collection for storing embeddings\"\"\"\n",
    "        try:\n",
    "            # Check if collection exists\n",
    "            if self.collection_name not in [\n",
    "                col.name for col in self.client.get_collections().collections\n",
    "            ]:\n",
    "                self.client.create_collection(\n",
    "                    collection_name=self.collection_name,\n",
    "                    vectors_config=VectorParams(\n",
    "                        size=self.vector_size, distance=Distance.COSINE\n",
    "                    ),\n",
    "                )\n",
    "                print(f\"Created collection: {self.collection_name}\")\n",
    "            else:\n",
    "                print(f\"Collection {self.collection_name} already exists\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating collection: {e}\")\n",
    "\n",
    "    def add_documents(self, documents: list[dict[str, str | dict[str, str]]]):\n",
    "        \"\"\"Add documents to Qdrant with embeddings\"\"\"\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                # Generate embedding for the text\n",
    "                vector=self.encoder.encode(doc[\"text\"]).tolist(),\n",
    "                payload={\n",
    "                    \"text\": doc[\"text\"],\n",
    "                    \"title\": doc[\"title\"],\n",
    "                    \"url\": doc[\"url\"],\n",
    "                    \"metadata\": doc[\"metadata\"],\n",
    "                },\n",
    "            )\n",
    "            for doc in documents\n",
    "        ]  # Create points for Qdrant\n",
    "        try:  # Batch upload to Qdrant\n",
    "            self.client.upsert(collection_name=self.collection_name, points=points)\n",
    "            print(f\"Successfully added {len(points)} documents to Qdrant\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents to Qdrant: {e}\")\n",
    "\n",
    "    def search_similar(self, query: str, limit: int = 5) -> list[dict]:\n",
    "        \"\"\"Search for similar documents using vector similarity\"\"\"\n",
    "        try:\n",
    "            return [\n",
    "                {\n",
    "                    \"score\": result.score,\n",
    "                    \"text\": result.payload[\"text\"],\n",
    "                    \"title\": result.payload[\"title\"],\n",
    "                    \"url\": result.payload[\"url\"],\n",
    "                    \"metadata\": result.payload[\"metadata\"],\n",
    "                }\n",
    "                for result in self.client.search(\n",
    "                    collection_name=self.collection_name,\n",
    "                    query_vector=self.encoder.encode(query).tolist(),\n",
    "                    limit=limit,\n",
    "                )\n",
    "            ]\n",
    "        except Exception as e:\n",
    "            print(f\"Error searching documents: {e}\")\n",
    "            return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSCoVRAGPipeline:\n",
    "    def __init__(self, qdrant_url: str = \"http://localhost:6333\"):\n",
    "        self.news_collector = NewsAPICollector()\n",
    "        self.rag_system = QdrantRAGSystem(qdrant_url=qdrant_url)\n",
    "\n",
    "    def collect_and_index_news(self, query: str = \"SARS-CoV-2 OR COVID-19\"):\n",
    "        \"\"\"Complete pipeline to collect news and index in RAG system\"\"\"\n",
    "        print(\"Fetching articles from News API...\")\n",
    "        if not (articles := self.news_collector.fetch_articles(query=query)):\n",
    "            print(\"No articles found\")\n",
    "            return\n",
    "\n",
    "        print(f\"Found {len(articles)} articles. Preprocessing...\")\n",
    "        processed_docs = self.news_collector.preprocess_articles(articles)\n",
    "        print(f\"Processed {len(processed_docs)} articles. Adding to Qdrant...\")\n",
    "        self.rag_system.add_documents(processed_docs)\n",
    "        print(\"Pipeline completed successfully!\")\n",
    "\n",
    "    def query_rag_system(self, user_query: str, num_results: int = 3):\n",
    "        \"\"\"Query the RAG system and return relevant information\"\"\"\n",
    "        print(f\"Searching for: {user_query}\")\n",
    "        if not (results := self.rag_system.search_similar(user_query, num_results)):\n",
    "            print(\"No relevant articles found\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\nFound {len(results)} relevant articles:\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        for i, result in enumerate(results, 1):\n",
    "            print(f\"\\n{i}. {result['title']}\")\n",
    "            print(f\"   Relevance Score: {result['score']:.3f}\")\n",
    "            print(f\"   Source: {result['metadata']['source_name']}\")\n",
    "            print(f\"   URL: {result['url']}\")\n",
    "            print(f\"   Preview: {result['text'][:200]}...\")\n",
    "            print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error creating collection: [WinError 10061] Nenhuma conexão pôde ser feita porque a máquina de destino as recusou ativamente\n",
      "Fetching articles from News API...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m pipeline = SARSCoVRAGPipeline()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect_and_index_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m      5\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat are the latest variants of SARS-CoV-2?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mCOVID-19 vaccine effectiveness\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mLong COVID symptoms and treatment\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      8\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSARS-CoV-2 transmission mechanisms\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m ]:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m60\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mSARSCoVRAGPipeline.collect_and_index_news\u001b[39m\u001b[34m(self, query)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Complete pipeline to collect news and index in RAG system\"\"\"\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mFetching articles from News API...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (articles := \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnews_collector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch_articles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m):\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNo articles found\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mNewsAPICollector.fetch_articles\u001b[39m\u001b[34m(self, query, language, sort_by, page_size, days_back)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfetch_articles\u001b[39m(\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m      8\u001b[39m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     days_back: \u001b[38;5;28mint\u001b[39m = \u001b[32m29\u001b[39m,\n\u001b[32m     13\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]]:\n\u001b[32m     14\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[33;03m    Fetch articles from News API for a specific query\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     to_date = \u001b[43mdatetime\u001b[49m.now()\n\u001b[32m     18\u001b[39m     from_date = to_date - timedelta(days=\u001b[38;5;28mmin\u001b[39m(days_back, \u001b[32m29\u001b[39m))  \u001b[38;5;66;03m# Ensure max 29 days\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "pipeline = SARSCoVRAGPipeline()\n",
    "pipeline.collect_and_index_news()\n",
    "\n",
    "for query in [\n",
    "    \"What are the latest variants of SARS-CoV-2?\",\n",
    "    \"COVID-19 vaccine effectiveness\",\n",
    "    \"Long COVID symptoms and treatment\",\n",
    "    \"SARS-CoV-2 transmission mechanisms\",\n",
    "]:\n",
    "    print(f\"\\n{'=' * 60}\")\n",
    "    pipeline.query_rag_system(query, num_results=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-indicium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
