{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "\n",
    "import openai\n",
    "from polars import read_parquet\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models as rest\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "df = read_parquet(\"../data/interim/srag_2019_2024.parquet\")\n",
    "DATA_COLS = {col: str(df[col].dtype) for col in df.columns}\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AIQueryAgent:\n",
    "    \"\"\"\n",
    "    Um agente que recebe perguntas em linguagem natural,\n",
    "    traduz para SQL, consulta um SQLite, sumariza os resultados\n",
    "    e busca notícias relacionadas no Qdrant.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        sqlite_path: str,\n",
    "        qdrant_url: str,\n",
    "        qdrant_collection: str,\n",
    "        embedding_model: str = \"text-embedding-ada-002\",\n",
    "        sql_model: str = \"gpt-3.5-turbo\",\n",
    "        summarization_model: str = \"gpt-3.5-turbo\",\n",
    "        final_model: str = \"gpt-3.5-turbo\",\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        :param sqlite_path: caminho para o arquivo .db do SQLite\n",
    "        :param qdrant_url: URL do servidor Qdrant\n",
    "        :param qdrant_collection: nome da coleção onde estão as notícias\n",
    "        :param embedding_model: modelo de embedding da OpenAI\n",
    "        :param sql_model: modelo de LLM para tradução NL→SQL\n",
    "        :param summarization_model: modelo de LLM para resumo de resultados\n",
    "        :param final_model: modelo de LLM para montar a resposta final\n",
    "        \"\"\"\n",
    "        self.sqlite_path = sqlite_path\n",
    "        self.qdrant = QdrantClient(url=qdrant_url)\n",
    "        self.qdrant_collection = qdrant_collection\n",
    "        self.embedding_model = embedding_model\n",
    "        self.sql_model = sql_model\n",
    "        self.summarization_model = summarization_model\n",
    "        self.final_model = final_model\n",
    "\n",
    "    def _execute_sql(self, sql: str) -> list[tuple]:\n",
    "        \"\"\"Executa a query SQL no banco SQLite e retorna todas as linhas.\"\"\"\n",
    "        conn = sqlite3.connect(self.sqlite_path)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(sql)\n",
    "        rows = cur.fetchall()\n",
    "        conn.close()\n",
    "        return rows\n",
    "\n",
    "    def _llm_chat(self, model: str, messages: list[dict]) -> str:\n",
    "        \"\"\"Chama o endpoint chat completions da OpenAI.\"\"\"\n",
    "        return (\n",
    "            openai.ChatCompletion.create(model=model, messages=messages)\n",
    "            .choices[0]\n",
    "            .message.content\n",
    "        )\n",
    "\n",
    "    def _natural_to_sql(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Traduz a pergunta em linguagem natural para uma consulta SQL válida.\n",
    "        \"\"\"\n",
    "        return self._llm_chat(\n",
    "            self.sql_model,\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Você é um gerador de consultas SQL para SQLite.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Transforme em SQL para SQLite:\\n\\n{question}\",\n",
    "                },\n",
    "            ],\n",
    "        ).strip()\n",
    "\n",
    "    def _summarize(self, question: str, results: list[tuple]) -> str:\n",
    "        \"\"\"\n",
    "        Gera um pequeno resumo explicativo dos resultados obtidos.\n",
    "        \"\"\"\n",
    "        return self._llm_chat(\n",
    "            self.summarization_model,\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Você é um assistente que resume resultados de tabelas.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": (\n",
    "                        \"Pergunta:\\n\"\n",
    "                        + question\n",
    "                        + \"\\n\\nResultados (como lista de tuplas):\\n\"\n",
    "                        + repr(results)\n",
    "                        + \"\\n\\nForneça um parágrafo curto explicando o que esses dados mostram.\"\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        ).strip()\n",
    "\n",
    "    def _get_embedding(self, text: str) -> list[float]:\n",
    "        \"\"\"Gera embedding para um texto usando a API da OpenAI.\"\"\"\n",
    "        return (\n",
    "            openai.Embedding.create(model=self.embedding_model, input=[text])\n",
    "            .data[0]\n",
    "            .embedding\n",
    "        )\n",
    "\n",
    "    def _search_news(self, embedding: list[float], limit: int = 5) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Busca notícias relacionadas no Qdrant, usando cosine similarity.\n",
    "        Retorna lista de hits (id, payload).\n",
    "        \"\"\"\n",
    "        return [\n",
    "            {\"id\": hit.id, \"score\": hit.score, \"payload\": hit.payload}\n",
    "            for hit in self.qdrant.search(\n",
    "                collection_name=self.qdrant_collection,\n",
    "                query_vector=embedding,\n",
    "                limit=limit,\n",
    "                with_payload=True,\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def ask(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Ciclo principal: recebe pergunta, executa SQL,\n",
    "        resume, busca notícias e devolve resposta final.\n",
    "        \"\"\"\n",
    "        sql = self._natural_to_sql(question)\n",
    "        summary = self._summarize(question, self._execute_sql(sql))\n",
    "        return self._llm_chat(\n",
    "            self.final_model,\n",
    "            [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"Você é um assistente que responde perguntas com base em dados tabulares e notícias relacionadas.\",\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"Pergunta: {question}\\n\\n\"\n",
    "                    f\"Consulta SQL gerada:\\n{sql}\\n\\n\"\n",
    "                    f\"Resumo dos dados:\\n{summary}\\n\\n\"\n",
    "                    f\"Notícias relacionadas (id e título):\\n\"\n",
    "                    + \"\\n\".join(\n",
    "                        f\"- {hit['id']}: {hit['payload'].get('title', '(sem título)')}\"\n",
    "                        for hit in self._search_news(self._get_embedding(summary))\n",
    "                    ),\n",
    "                },\n",
    "            ],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AIQueryAgent(\n",
    "    sqlite_path=\"../data/interim/srag_2019_2024.db\",\n",
    "    qdrant_url=\"http://localhost:6333\",\n",
    "    qdrant_api_key=None,\n",
    "    qdrant_collection=\"noticias\",\n",
    ")\n",
    "agent.ask(\"what was the month with more cases of sars in 2019\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-indicium",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
